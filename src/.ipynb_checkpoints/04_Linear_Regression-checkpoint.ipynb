{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s._set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 03_Batch_Processing.ipynb](./03_Batch_Processing.ipynb)\n",
    "\n",
    "- Explored linear algebra in `numpy` for batch processing of samples\n",
    "- Saw massive speedups! Use array operations whenever possible\n",
    "\n",
    "## Today\n",
    "- Cost functions and formulating a machine learning task as an optimization problem\n",
    "- Understand linear regression \n",
    "- Learn how linear regression can learn non-linear functions using feature maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Steady-State One-Dimensional Heat Conduction\n",
    "\n",
    "Fourier's law of heat conduction for a bar of constant cross-sectional area connected between two reservoirs in the steady-state limit gives a simple differential equation for the spatial dependence of the temperature $T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^2 T(x)}{d x^2} &= 0 \\\\\n",
    "\\frac{d T(x)}{dx} &= w \\\\\n",
    "T(x) &= w x + b \n",
    "\\end{align}\n",
    "\n",
    "Load experimental data from `../data/rod_temperature.dat` using the very convenient `np.loadtxt()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/rod_temperature.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,T,ΔT = np.loadtxt('../data/rod_temperature.dat', unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,T, 'o')\n",
    "plt.xlabel('x / m')\n",
    "plt.ylabel('T / °C');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a linear relationship, let's try to fit some lines by *eye*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [800,941,1000]\n",
    "b = [0,4.8,10]\n",
    "x_fit = np.linspace(np.min(x),np.max(x),100)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(10,3))\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "for i in range(len(w)):\n",
    "    ax[0].plot(x_fit,w[i]*x_fit + b[i], color=colors[i+1])\n",
    "    ax[1].plot(w[i],b[i], 'o', color=colors[i+1])\n",
    "    \n",
    "ax[0].plot(x,T, 'o', ms=6)\n",
    "ax[0].set_xlabel('x / m')\n",
    "ax[0].set_ylabel('T / °C')\n",
    "ax[0].set_title('Data Space')\n",
    "\n",
    "ax[1].set_xlabel('w / (°C/m)')\n",
    "ax[1].set_ylabel('b / °C')\n",
    "ax[1].set_title('Weight Space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Want to predict a scalar $T$ as a function of scalar $x$ given a dataset of pairs $\\{(x^{(n)},T^{(n)})\\}_{i=1}^N$.  Here the $x^{(n)}$ are inputs and the $T^{(n)}$ are targets or observations. From physics, we have a model:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x) = w x + b\n",
    "\\end{equation}\n",
    "\n",
    "i.e. $F^{(n)} = w x^{(n)} + b$.\n",
    "\n",
    "We can think of this as the simplest possible **shallow** neural network (no hidden layer) and non non-linearity, i.e. $f(x) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[r'$x$'],[r'$F(x) = wx + b$']]\n",
    "ml4s.draw_network([1,1],node_labels=labels, weights=['w'], biases=['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to *learn* the **parameters** (weight $w$ and bias $b$) based on the **prediction** $F$ (here a linear function).  We will do this by minimizing (optimizing) a **loss** function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\frac{1}{2} \\lvert \\lvert F - T \\rvert \\rvert^2\n",
    "\\end{equation}\n",
    "\n",
    "which quantifies the goodness of fit over our **hypothesis** space (all values of the parameters).  $F-T$ is the residual, we want to make this as small as possible, which we can do by computing the **Cost** function, the loss function averaged over all training examples (input data):\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\lvert \\lvert F^{(n)} - T^{(n)} \\rvert \\rvert^2\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "Let's use what we learned last time about batch processing to look at this loss function. Here, our input samples are the individual values of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a specific hypothesis (i.e. individual values of w and b)\n",
    "C_hyp = []\n",
    "for i in range(len(w)):\n",
    "    F = np.dot(x,w[i]) + b[i]\n",
    "    C_hyp.append(0.5*np.average((F-T)**2))\n",
    "print(C_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can do this over the entire space of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 100 \n",
    "weights,biases = np.meshgrid(np.linspace(700,1200,grid_size),np.linspace(-1,11,grid_size))\n",
    "C = np.zeros_like(weights)\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        F = np.dot(x,weights[i,j]) + biases[i,j]\n",
    "        C[i,j] = 0.5*np.average((F-T)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(weights,biases,C, cmap='Spectral_r', levels=100)\n",
    "\n",
    "for i in range(len(w)):\n",
    "    plt.plot(w[i],b[i], 'o', color=colors[i+1])\n",
    "\n",
    "plt.xlabel('w / (°C/m)')\n",
    "plt.ylabel('b / °C')\n",
    "plt.colorbar(label='Cost Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(weights, biases,C , rstride=1, cstride=1, cmap='Spectral_r', \n",
    "                       linewidth=0, antialiased=True, rasterized=True)\n",
    "ax.set_xlabel('w / (°C/m)',labelpad=8)\n",
    "ax.set_ylabel('b / °C',labelpad=8)\n",
    "ax.set_zlabel('C(w,b)',labelpad=8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving beyond 1 dimension\n",
    "\n",
    "Note that while we have looked at a simple 1D example, there is nothing stopping us from generalizing our model to an arbitrary number of  $D$ dimensions (i.e. think number of input neurons).  We don't even need to change our notation if we use matrix vector multiplication\n",
    "\n",
    "\\begin{align}\n",
    "F &= \\sum_j w_j x_j + b \\\\\n",
    "&= \\vec{w}^{\\sf T} \\vec{x} + b\n",
    "\\end{align}\n",
    "\n",
    "where $\\vec{w}^{\\sf T} = (w_1,\\dots, w_{D})$ and $\\vec{x} = (x_1,\\dots, x_{D})$.   Note that our code `np.dot(x,w)+b` doesn't even need to change!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = [D,1]\n",
    "labels = [[r'$x_{' + f'{i}' + r'}$' for i in range(1,N[0]+1)],[r'$F$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[[r'$w_{' + f'{i}' + r'}$' for i in range(1,N[0]+1)]], biases=['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can perform the same batch processing as above, where now each **row** of $\\mathbf{w}$ corresponds to a training example.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\cdot \\mathbf{w} + b \\mathbb{1} = \\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^{\\mathsf{T}} \\cdot \\mathbf{x}^{(1)} + b \\\\\n",
    "\\mathbf{w}^{\\mathsf{T}} \\cdot \\mathbf{x}^{(2)} + b \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^{\\mathsf{T}} \\cdot \\mathbf{x}^{(N)} + b\n",
    "\\end{array}\\right) = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "F^{(1)} \\\\\n",
    "F^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "F^{(N)}\n",
    "\\end{array}\n",
    "\\right) = \\vec{F}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\mathbb{1}$ is the identify matrix.  We can simplify our notation even further by noticing that we can incorporate the bias into the weight by tacking on a dummy input $x_0$ that always takes the value $1$ such that $w_0$ can be interpreted as the weight:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} &= (1,x_1,\\dots, x_D)^{\\mathsf{T}} \\\\\n",
    "\\mathbf{W} &= (b,w_1,\\dots, w_D)^{\\mathsf{T}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [D+1,1]\n",
    "labels = [[r'$x_{' + f'{i}' + r'}$' for i in range(N[0])],[r'$F$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[[r'$w_{' + f'{i}' + r'}$' for i in range(N[0])]], biases=[' '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the squared error cost across the entire data set:\n",
    "\n",
    "\\begin{align}\n",
    "C &= \\frac{1}{2N} \\lvert\\lvert \\vec{F} - \\vec{T}\\rvert\\rvert^2 \\\\\n",
    "&= \\frac{1}{2N} \\lvert\\lvert \\mathbf{X} \\cdot \\mathbf{W} - \\vec{T} \\rvert\\rvert^2\n",
    "\\end{align}\n",
    "\n",
    "without modifying any of our python code.\n",
    "\n",
    "## Solving the Optimization Problem\n",
    "\n",
    "Recall, we are interested in finding the values of the weights and biases which minimize the cost function.  For the case of linear regression this can be done explicitly via calculus.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_j} = \\frac{1}{N} \\sum_{n=1}^{N}\\left(F^{(n)} - T^{(n)} \\right) x_j^{(n)} = \\left \\langle \\left(F^{(n)} - T^{(n)} \\right) x_j^{(n)} \\right\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "The minimum occurs when this equation is set to zero, which offers a convenient closed-form solution (you will derive this in the homework):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W} = \\left(\\mathbf{X}^{\\sf T} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\sf T} \\vec{T}\n",
    "\\end{equation}\n",
    "\n",
    "We can check this for our simple example of the rod temperature above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([len(x),2])\n",
    "X[:,0] = 1\n",
    "X[:,1] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(),X)),X.transpose()),T)\n",
    "C_opt = 0.5*np.average((np.dot(X,W_opt)-T)**2)\n",
    "\n",
    "print(f'W_opt = {W_opt}')\n",
    "print(f'C_opt = {C_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be compared with the `np.polyfit` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(x,T,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They have chosen to pack their extra dimension at the end! We can also compare the cost with a global optimization of the cost function computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Let's plot the optimal linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,T, 'o', label='Exp. Data')\n",
    "\n",
    "X_fit = np.zeros([x_fit.shape[0],2])\n",
    "X_fit[:,0] = 1\n",
    "X_fit[:,1] = x_fit\n",
    "plt.plot(x_fit, np.dot(X_fit,W_opt), color=colors[0], label='Linear Regression' )\n",
    "plt.xlabel('x / m')\n",
    "plt.ylabel('T / °C')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Time: What do we do when we can't minimize by hand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml4s)",
   "language": "python",
   "name": "ml4s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
