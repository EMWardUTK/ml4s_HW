{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Feature Maps for Non-Linear Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s._set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 04_Linear_Regression.ipynb](./04_Linear_Regression.ipynb)\n",
    "\n",
    "- Cost functions and formulating a machine learning task as an optimization problem\n",
    "- Understand linear regression \n",
    "\n",
    "## Today\n",
    "\n",
    "- Learn how linear regression can learn non-linear functions using feature maps.\n",
    "- Understanding model complexity and the bias-variance tradeoff\n",
    "\n",
    "We can generalize everything we learned about linear regression to non-linear models composed of a linear combination of non-linear parts, *i.e.* feature maps.  Let us change our notation slightly (as we would like to be fully general).  As before, our goal is to predict a scalar *target* $y$ as a function of scalar $x$ given a dataset of pairs $\\mathcal{D} = \\{(x^{(n)},y^{(n)})\\}_{n=1}^N$.  Here the $x^{(n)}$ are inputs and the $y^{(n)}$ are targets or observations.  We now let our model be more general:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x,\\vec{w}) = \\vec{w}^{\\sf T} \\vec{\\varphi}(x)\n",
    "\\end{equation}\n",
    "\n",
    "which is **linear** in the *weights*: \n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w} = \\left( \\begin{array}{c}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_{M-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "but can be **non-linear** in $x$ depending on the basis functions features (think pre-processing layer):\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\varphi}(x) = \\left( \\begin{array}{c}\n",
    "\\varphi_0(x) \\\\\n",
    "\\varphi_1(x) \\\\\n",
    "\\vdots \\\\\n",
    "\\varphi_{M-1}(x)\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "### Examples of Basis Functions\n",
    "1. **Identity Transformation:** $\\varphi_j(x) = x$; this is just the linear regression we have already seen.\n",
    "2. **Polynomial Decomposition:** $\\varphi_j(x) = x^j$; note that the basis functions are global, they behave non-trivially on the entire domain of $x$.\n",
    "3. **Sigmoid Basis:** $\\varphi_j(x) = \\sigma((x-\\mu_j)/s)$ where $\\sigma(z) = 1/(1-\\mathrm{e}^{-z})$; note that this can affect a local region of the domain non-trivially.\n",
    "4. **Fourier  Basis:** $\\varphi_j(x) = \\mathrm{e}^{i 2\\pi x_j}$; action is local in the frequency domain.\n",
    "\n",
    "Consider a 3rd order polynomial in $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [1,4,1]\n",
    "labels = [[r'$x$'],['1','$x$','$x^2$','$x^3$'],[r'$F(x,\\vec{w})$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[['' for i in range(N[1])],[f'$w_{i}$' for i in range(N[1])]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our previous derivations go through unchanged provided we replace our design matrix $\\mathbf{X}$ with a new feature matrix $\\mathbf{\\Phi}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi} = \\left( \\begin{array}{cccc}\n",
    "        \\varphi_0(x^{(1)}) & \\varphi_1(x^{(1)}) & \\cdots & \\varphi_{M-1}(x^{(1)}) \\\\\n",
    "        \\varphi_0(x^{(2)}) & \\varphi_1(x^{(2)}) & \\cdots & \\varphi_{M-1}(x^{(2)}) \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "\\varphi_0(x^{(N)}) & \\varphi_1(x^{(1)}) & \\cdots & \\varphi_{M-1}(x^{(N)}) \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "which, when minimizing the squared error costs across the entire dataset:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\lvert \\lvert F^{(n)}(x^{(n)},\\vec{w}) - y^{(n)} \\rvert \\rvert^2\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "yields the optimal parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}^\\ast = \\left(\\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\sf T} \\vec{y}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{y}$ is the vector of targets (corresponding to each sample).\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Load data from disk `../include/poly_regression.dat`\n",
    "\n",
    "\n",
    "<!--\n",
    "x = np.linspace(0,1,10)\n",
    "header = f\"{'x':>13s}\\t{'y':>15s}\"\n",
    "data_out = np.column_stack([x,np.sin(2*np.pi*x)+np.random.normal(loc=0,scale=0.15,size=x.size)])\n",
    "np.savetxt('../data/poly_regression.dat', data_out,fmt='% 15.8e', header=header, delimiter='\\t')\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/poly_regression.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression.dat',unpack=True)\n",
    "plt.plot(x,y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_order = 3\n",
    "Φ = np.zeros([len(x),poly_order+1])\n",
    "for j in range(Φ.shape[1]):\n",
    "    Φ[:,j] = x**j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = np.dot(np.dot(np.linalg.inv(np.dot(Φ.T,Φ)),Φ.T),y)\n",
    "C_opt = 0.5*np.average((np.dot(Φ,W_opt)-y)**2)\n",
    "\n",
    "print(f'W_opt = {W_opt}')\n",
    "print(f'C_opt = {C_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can compare this with the `np.polyfit` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(x,y,poly_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data')\n",
    "\n",
    "x_fit = np.linspace(np.min(x),np.max(x),100)\n",
    "Φ_fit = np.zeros([len(x_fit),poly_order+1])\n",
    "for j in range(Φ.shape[1]):\n",
    "    Φ_fit[:,j] = x_fit**j\n",
    "\n",
    "plt.plot(x_fit,np.dot(Φ_fit,W_opt),'-', color=colors[0], label='fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Model Complexity\n",
    "\n",
    "Here we have just guessed the order of the polynomial for regression.  We can systematically explore the fit as a function of the order of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data', color=colors[0])\n",
    "plt.plot(x_fit,np.sin(2*np.pi*x_fit), color=colors[0], ls='--', label=r'$\\sin(2\\pi x)$')\n",
    "\n",
    "W_opt = []\n",
    "C_opt = []\n",
    "\n",
    "poly_order = [0,4,9]\n",
    "\n",
    "for n,cpoly_order in enumerate(poly_order):\n",
    "    Φ = np.zeros([len(x),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x**j\n",
    "        \n",
    "    W_opt.append(np.dot(np.dot(np.linalg.inv(np.dot(Φ.T,Φ)),Φ.T),y))\n",
    "    C_opt.append(0.5*np.average((np.dot(Φ,W_opt[n])-y)**2))\n",
    "    \n",
    "    Φ_fit = np.zeros([len(x_fit),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ_fit[:,j] = x_fit**j\n",
    "\n",
    "    plt.plot(x_fit,np.dot(Φ_fit,W_opt[n]),'-', color=colors[2*n+1], label=f'order = {poly_order[n]}; cost = {C_opt[n]:.1e}' )\n",
    "        \n",
    "    \n",
    "plt.legend(loc=(1,0.0))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we can observe two extremes:\n",
    "\n",
    "**Underfitting:** Model is too simple – does not fit the data.\n",
    "\n",
    "**Overfitting:** Model is too complex – fits perfectly but does not generalize.\n",
    "\n",
    "We can explore this behavior systematically by employing knowledge from **Statistical Learning Theory** (see [Online Course](https://work.caltech.edu/telecourse) and associated textbook by Yaser Abu-Mostafa).\n",
    "\n",
    "Let us consider our previous example (noisy $\\sin(2\\pi x)$) but with much more data ($N = 1000$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{D} = \\{(x^{(n)},y^{(n)})\\}_{n=1}^{1000}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The **first step** is to randomly divide our data set $\\mathcal{D}$ into two mutually exclusive groups $\\mathcal{D}_{\\rm train}$ and $\\mathcal{D}_{\\rm test}$.  Can do this with built-in `numpy` functions.  No hard-and-fast rule for the relative sizes (we are encountering one of our first *hyperparameters*), 90% is a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression_long.dat',unpack=True)\n",
    "\n",
    "N_train = int(0.9*x.size)\n",
    "indices = np.random.permutation(x.shape[0])\n",
    "training_idx, test_idx = indices[:N_train], indices[N_train:]\n",
    "x_train,x_test = x[training_idx],x[test_idx]\n",
    "y_train,y_test = y[training_idx],y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is often simpler to use pre-canned versions from machine-learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression_long.dat',unpack=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o', ms=4, label='Training Data')\n",
    "plt.plot(x_test,y_test, 's', ms=4, label='Testing Data')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then minimize the cost function using **only** data in the training set.  This gives us two different measurements of error:\n",
    "\n",
    "#### In-Sample (training) Error\n",
    "\\begin{equation}\n",
    "E_{\\rm in} = \\mathcal{C}\\left(\\vec{y}_{\\rm train},F(\\vec{x}_{\\rm train}, \\vec{w}^\\ast)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "#### Out-of-sample (testing) Error\n",
    "\\begin{equation}\n",
    "E_{\\rm out} = \\mathcal{C}\\left(\\vec{y}_{\\rm test},F(\\vec{x}_{\\rm test}, \\vec{w}^\\ast)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "It is **almost always the case** that out-of-sample error is greater than in-sample error.\n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm out} \\ge E_{\\rm in}.\n",
    "\\end{equation}\n",
    "\n",
    "Splitting the data into mutually exclusive training and test sets provides an unbiased estimate for the predictive performance of the model — this is known as cross-validation in the ML and statistics literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = []\n",
    "E_in,E_out = [],[]\n",
    "\n",
    "poly_order = np.arange(20)\n",
    "\n",
    "for n,cpoly_order in enumerate(poly_order):\n",
    "    \n",
    "    # training\n",
    "    Φ = np.zeros([len(x_train),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x_train**j\n",
    "    W_opt.append(np.dot(np.dot(np.linalg.inv(np.dot(Φ.T,Φ)),Φ.T),y_train))\n",
    "    \n",
    "    # in-sample (training) error\n",
    "    E_in.append(0.5*np.average((np.dot(Φ,W_opt[n])-y_train)**2))\n",
    "    \n",
    "    # out-of-sample (testing) error\n",
    "    Φ = np.zeros([len(x_test),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x_test**j\n",
    "    E_out.append(0.5*np.average((np.dot(Φ,W_opt[n])-y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(poly_order,E_in, label=r'$E_{\\rm in}$')\n",
    "plt.plot(poly_order,E_out, label=r'$E_{\\rm out}$')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Polynomial Order (Model Complexity)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Least Squares\n",
    "\n",
    "When we enter a regime of **overfitting** one consistent observation is that the fitting parameters tend to grow very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ml4s.get_linear_colors('Spectral_r',len(poly_order))\n",
    "for i,cW in enumerate(W_opt):\n",
    "    plt.plot(np.abs(cW), '-o', ms=5, lw=1, color=colors[i])\n",
    "plt.yscale('symlog')\n",
    "plt.ylabel(r'$w_j$')\n",
    "plt.xlabel(r'$j$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to address this is just to punish large values of the weights in a modified cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{\\rm ridge}(\\vec{w}) = \\frac{1}{2}|| \\vec{F}(\\vec{w})-\\vec{y}||^2 + \\frac{\\lambda}{2} ||\\vec{w}||^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is a *regularization constant*.  This is another hyperparameters that we need to choose by investigation.  Different powers of the regression term can act differently ($||\\vec{w}||$ is called *lasso* regression in the statistics literature.  For the choice above, our equation for the optimal parameters above are modified to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}^\\ast = \\left(\\lambda \\mathbb{1} + \\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\sf T} \\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbb{1}$ is the $M\\times M$ identity matrix.  Note the fact that the regularizer now prevents divergences if $\\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}$ becomes singular.  We can easily modify our code above to use this regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-success\">\n",
    "<h2> Team Programming challenge: Ridge Regression </h2>\n",
    "\n",
    "Adapt our analysis above to use use ridge regression.  Play around with different values of $\\lambda$ to observe their effects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
